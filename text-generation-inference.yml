---
apiVersion: v1
kind: Secret
metadata:
  name: tgi
type: Opaque
data:
  name: HUGGING_FACE_HUB_TOKEN
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: text-generation-inference
spec:
  storageClassName: nfs
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 200G

# llama-3-2-11b-vision-instruct
---
apiVersion: v1
kind: Service
metadata:
  name: tgi-llama-3-2-11b-vision-instruct
  labels:
    app: tgi-llama-3-2-11b-vision-instruct
spec:
  type: ClusterIP
  ports:
  - port: 80
  selector:
    app: tgi-llama-3-2-11b-vision-instruct
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tgi-llama-3-2-11b-vision-instruct
  labels:
    app: tgi-llama-3-2-11b-vision-instruct
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tgi-llama-3-2-11b-vision-instruct
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: tgi-llama-3-2-11b-vision-instruct
    spec:
      # This will need to be changd to accomodate your GPU Servers
      nodeSelector:
        nvidia.com/gpu.product: NVIDIA-H100-NVL
      imagePullSecrets:
      - name: regcred
      containers:
      - image: ghcr.io/huggingface/text-generation-inference:3.2.0
        name: tgi-llama-3-2-11b-vision-instruct
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - 'curl -f -s http://localhost/health'
          initialDelaySeconds: 5
          periodSeconds: 30
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: tgi
              key: HUGGING_FACE_HUB_TOKEN
        - name: MODEL_ID
          value: "meta-llama/Llama-3.2-11B-Vision-Instruct"
        - name: PAYLOAD_LIMIT
          value: "50000000"
        - name: PREFIX_CACHING
          value: "0"
        - name: MAX_INPUT_TOKENS
          value: "127999"
        - name: MAX_TOTAL_TOKENS
          value: "128000"
        - name: MAX_BATCH_PREFILL_TOKENS
          value: "127999"
        ports:
        - containerPort: 80
          name: http
        volumeMounts:
        - name: storage
          mountPath: /data/
          subPath: "tgi-llama-3-2-11b-vision-instruct"
        - name: shm
          mountPath: "/dev/shm"
        resources:
          limits:
            nvidia.com/gpu: 1
      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: text-generation-inference
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 64Gi

